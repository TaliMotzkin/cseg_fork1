{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baiyn-5jdy2W"
      },
      "source": [
        "##Basic Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2R0SkyHcWR5",
        "outputId": "38519cd1-6516-4410-b3e7-cb36b99e3042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone cseg into /content/ folder\n",
        "!git clone https://github.com/noellelaw/cseg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky8W1fciccCM",
        "outputId": "ab3c97ce-b308-4de2-8f6c-fb0001cc64ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cseg'...\n",
            "remote: Enumerating objects: 426, done.\u001b[K\n",
            "remote: Counting objects: 100% (345/345), done.\u001b[K\n",
            "remote: Compressing objects: 100% (215/215), done.\u001b[K\n",
            "remote: Total 426 (delta 132), reused 315 (delta 120), pack-reused 81\u001b[K\n",
            "Receiving objects: 100% (426/426), 6.66 MiB | 4.88 MiB/s, done.\n",
            "Resolving deltas: 100% (148/148), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_yi9ZiA6i7e",
        "outputId": "49123069-9160-4cc2-f56c-5cc8d174bef3",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cseg\n",
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html, https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.29.36)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
            "Collecting timm (from -r requirements.txt (line 4))\n",
            "  Downloading timm-0.9.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.8.0)\n",
            "Collecting wandb (from -r requirements.txt (line 6))\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire (from -r requirements.txt (line 7))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.7.0.72)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.3)\n",
            "Collecting ftfy (from -r requirements.txt (line 10))\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.65.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.6.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.1+cu113 (from versions: 1.11.0, 1.11.0+cu113, 1.12.0, 1.12.0+cu113, 1.12.1, 1.12.1+cu113, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.1+cu113\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting wandb\n",
            "  Using cached wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=12454dc5ec1b2aadf745675ce3d926257aaf2eb2bfa94207ca21fdf57cee760d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\n",
            "Collecting timm\n",
            "  Using cached timm-0.9.5-py3-none-any.whl (2.2 MB)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 timm-0.9.5\n",
            "Collecting ftfy\n",
            "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "#@title Install CSeg\n",
        "%cd /content/cseg/\n",
        "!pip install -r requirements.txt\n",
        "!pip install wandb\n",
        "!pip install timm\n",
        "!pip install ftfy\n",
        "import multiprocessing as mp\n",
        "try:\n",
        "    import detectron2\n",
        "except:\n",
        "    import os\n",
        "    os.system('pip install git+https://github.com/facebookresearch/detectron2.git')\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from open_vocab_seg import add_ovseg_config\n",
        "from open_vocab_seg.utils import VisualizationDemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xknp57qneJxr"
      },
      "outputs": [],
      "source": [
        "#@title Import common libraries\n",
        "\n",
        "import torch\n",
        "import requests\n",
        "import sys\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import math\n",
        "from time import perf_counter\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as mplc\n",
        "import matplotlib.figure as mplfigure\n",
        "from torchvision import transforms\n",
        "import colorsys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlawIO88aJk9"
      },
      "source": [
        "##Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUit91WbgOuc",
        "outputId": "3fad9bff-3df6-4582-859c-91b0eebaf9a4",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: DETECTRON2_DATASETS=/content/drive/MyDrive/datasets/\n",
            "env: WANDB_MODE=offline\n"
          ]
        }
      ],
      "source": [
        "#@title Set up environment\n",
        "# Path to where your training datasets are\n",
        "%env DETECTRON2_DATASETS=/content/drive/MyDrive/datasets/ #@param{DETECTRON2_DATASETS: \"string\"}\n",
        "%env WANDB_MODE=offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBgf9syrPbQt",
        "outputId": "33077fe7-c542-4566-8577-7c69e4e0a819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command Line Args: Namespace(config_file='/content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:49152', opts=[])\n",
            "Loading config /content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[07/26 03:23:12 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[07/26 03:23:13 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]\n",
            "numpy                            1.22.4\n",
            "detectron2                       0.6 @/usr/local/lib/python3.10/dist-packages/detectron2\n",
            "Compiler                         GCC 11.3\n",
            "CUDA compiler                    CUDA 11.8\n",
            "detectron2 arch flags            7.5\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.0.1+cu118 @/usr/local/lib/python3.10/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla T4 (arch=7.5)\n",
            "Driver version                   525.105.17\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           8.4.0\n",
            "torchvision                      0.15.2+cu118 @/usr/local/lib/python3.10/dist-packages/torchvision\n",
            "torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.7.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.8\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.7\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[07/26 03:23:13 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:49152', opts=[])\n",
            "\u001b[32m[07/26 03:23:13 detectron2]: \u001b[0mContents of args.config_file=/content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml:\n",
            "\u001b[38;5;197mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mOVSeg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mD2SwinTransformer\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m18\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m4\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m8\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m16\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m32\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186m/content/drive/MyDrive/ov-seg2/ovseg_swinbase_vitL14_ft_mpt.pth\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m123.675\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m116.280\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m103.530\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m58.395\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.120\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.375\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mOpenVocabMaskFormerHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m171\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# number of categories in training set\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBEDDING_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBED_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# not used, hard-coded\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMASK_FORMER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRANSFORMER_IN_FEATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCLIP_ADAPTER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTEXT_TEMPLATES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mvild\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_MODEL_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mViT-L/14\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_FILL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mmean\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_EXPAND_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_THR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# choose the foreground objects\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_MATTING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# use soft background, default not used\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_PROMPT_DEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_PROMPT_FWD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# use mask prompt during forward\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mREGION_RESIZED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# resize to the input of clip, e.g., 224\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_ENSEMBLE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# use ensemble of two classification branches\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_ENSEMBLE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;197mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;141m(\"coco_2017_train_stuff_sem_seg\",)\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(\"ade20k_sem_seg_val\",)\u001b[39m\n",
            "\u001b[38;5;197mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6e-5\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m120000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1e-6\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1500\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mWarmupPolyLR\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m500\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTEST_IMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mfull_model\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;197mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81m!!python/object/apply:eval\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186m[int(x\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m0.1\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m640)\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186mfor\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186mx\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186min\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186mrange(5,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m21)]\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mchoice\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mabsolute\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(640,\u001b[39m\u001b[38;5;141m \u001b[39m\u001b[38;5;141m640)\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;242m# used in dataset mapper\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mRGB\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m256\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m384\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m512\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m640\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m768\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m896\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;197mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;197mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\n",
            "\u001b[32m[07/26 03:23:13 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;197mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;197mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;197mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSAMPLE_PER_CLASS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSAMPLE_SEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_val\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcoco_2017_train_stuff_sem_seg\u001b[39m\n",
            "\u001b[38;5;197mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;197mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m448\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m576\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m704\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m832\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m960\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1088\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1152\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1216\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1280\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;197mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mD2SwinTransformer\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCLIP_ADAPTER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_ENSEMBLE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_ENSEMBLE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_MODEL_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mViT-L/14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_EXPAND_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_FILL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmean\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_MATTING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_PROMPT_DEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_PROMPT_FWD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_THR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPREDEFINED_PROMPT_TEMPLATES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141ma\u001b[39m\u001b[38;5;141m \u001b[39m\u001b[38;5;141mphoto\u001b[39m\u001b[38;5;141m \u001b[39m\u001b[38;5;141mof\u001b[39m\u001b[38;5;141m \u001b[39m\u001b[38;5;141ma\u001b[39m\u001b[38;5;141m \u001b[39m\u001b[38;5;141m{}.\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROMPT_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mREGION_RESIZED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTEXT_TEMPLATES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mvild\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMASK_FORMER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRANSFORMER_IN_FEATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mOVSeg\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mdeeplab\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBEDDING_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBED_HIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBED_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mOpenVocabMaskFormerHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m171\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBasePixelDecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM_INDICES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m16\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROJECTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROJECT_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m/content/drive/MyDrive/ov-seg2/ovseg_swinbase_vitL14_ft_mpt.pth\u001b[39m\n",
            "\u001b[38;5;197mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;197mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;197mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6.0e-05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m500\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupPolyLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m120000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTEST_IMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0e-06\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1500\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mDENSE_CRF\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSLIDING_OVERLAP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6666666666666666\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSLIDING_TILE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m224\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSLIDING_WINDOW\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;197mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;197mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;197mWANDB\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPROJECT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mopen_vocab_seg\u001b[39m\n",
            "\n",
            "\u001b[32m[07/26 03:23:13 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[07/26 03:23:13 d2.utils.env]: \u001b[0mUsing a generated random seed 13919132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[07/26 03:23:36 d2.engine.defaults]: \u001b[0mModel:\n",
            "OVSeg(\n",
            "  (backbone): D2SwinTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.013)\n",
            "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.026)\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.039)\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
            "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.052)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.065)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.078)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.091)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.104)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.117)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.130)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.143)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.157)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.170)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.183)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.196)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (12): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.209)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (13): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.222)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (14): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.235)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (15): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.248)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (16): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.261)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (17): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.274)\n",
            "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.287)\n",
            "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.300)\n",
            "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (sem_seg_head): OpenVocabMaskFormerHead(\n",
            "    (pixel_decoder): BasePixelDecoder(\n",
            "      (adapter_1): Conv2d(\n",
            "        128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_2): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_3): Conv2d(\n",
            "        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_4): Conv2d(\n",
            "        1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (predictor): OpenVocabTransformerPredictor(\n",
            "      (pe_layer): PositionEmbeddingSine()\n",
            "      (transformer): Transformer(\n",
            "        (encoder): TransformerEncoder(\n",
            "          (layers): ModuleList()\n",
            "        )\n",
            "        (decoder): TransformerDecoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x TransformerDecoderLayer(\n",
            "              (self_attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (multihead_attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (dropout3): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (input_proj): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (class_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): Linear(in_features=1024, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): SetCriterion(\n",
            "    (matcher): Matcher HungarianMatcher\n",
            "        cost_class: 1\n",
            "        cost_mask: 20.0\n",
            "        cost_dice: 1.0\n",
            "  )\n",
            "  (clip_adapter): ClipEnsembler(\n",
            "    (clip_model_reg): CLIP(\n",
            "      (visual): VisionTransformer(\n",
            "        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
            "        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (transformer): Transformer(\n",
            "          (resblocks): Sequential(\n",
            "            (0): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (1): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (2): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (3): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (4): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (5): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (6): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (7): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (8): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (9): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (10): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (11): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (12): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (13): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (14): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (15): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (16): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (17): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (18): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (19): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (20): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (21): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (22): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (23): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mask_pool): AvgPool2d(kernel_size=14, stride=14, padding=0)\n",
            "      )\n",
            "      (transformer): Transformer(\n",
            "        (resblocks): Sequential(\n",
            "          (0): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (6): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (7): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (8): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (9): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (10): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (11): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (token_embedding): Embedding(49408, 768)\n",
            "      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (clip_model): CLIP(\n",
            "      (visual): VisionTransformer(\n",
            "        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
            "        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (transformer): Transformer(\n",
            "          (resblocks): Sequential(\n",
            "            (0): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (1): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (2): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (3): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (4): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (5): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (6): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (7): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (8): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (9): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (10): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (11): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (12): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (13): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (14): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (15): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (16): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (17): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (18): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (19): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (20): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (21): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (22): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (23): ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mask_pool): AvgPool2d(kernel_size=14, stride=14, padding=0)\n",
            "      )\n",
            "      (transformer): Transformer(\n",
            "        (resblocks): Sequential(\n",
            "          (0): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (6): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (7): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (8): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (9): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (10): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (11): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (token_embedding): Embedding(49408, 768)\n",
            "      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (text_templates): VILDPromptExtractor()\n",
            "    (mask_decoder): MaskDecoder(\n",
            "      (transformer): TwoWayTransformer(\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x TwoWayAttentionBlock(\n",
            "            (self_attn): Attention(\n",
            "              (q_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (k_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (out_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
            "            )\n",
            "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (cross_attn_token_to_image): Attention(\n",
            "              (q_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (k_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (out_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
            "            )\n",
            "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): MLPBlock(\n",
            "              (lin1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "              (lin2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (norm4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (cross_attn_image_to_token): Attention(\n",
            "              (q_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (k_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "              (out_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (final_attn_token_to_image): Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "          (k_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        )\n",
            "        (norm_final_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (mask_tokens): Embedding(1, 1024)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "clip_adapter.clip_model.visual.proj\n",
            "clip_adapter.clip_model.visual.ln_post.weight\n",
            "clip_adapter.clip_model.visual.ln_post.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.q_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.q_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.k_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.k_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.v_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.v_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.out_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.self_attn.out_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm1.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm1.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm2.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm2.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.mlp.lin1.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.mlp.lin1.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.mlp.lin2.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.mlp.lin2.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm3.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm3.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm4.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.norm4.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.q_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.q_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.k_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.k_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.v_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.v_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.out_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.self_attn.out_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm1.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm1.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm2.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm2.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.mlp.lin1.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.mlp.lin1.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.mlp.lin2.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.mlp.lin2.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm3.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm3.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm4.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.norm4.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight\n",
            "clip_adapter.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias\n",
            "clip_adapter.mask_decoder.transformer.norm_final_attn.weight\n",
            "clip_adapter.mask_decoder.transformer.norm_final_attn.bias\n",
            "clip_adapter.mask_decoder.mask_tokens.weight\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 03:23:41 d2.data.datasets.coco]: \u001b[0mDirectory /content/drive/MyDrive/datasets/coco/train2017 and /content/drive/MyDrive/datasets/coco/stuffthingmaps_detectron2/train2017 has 118287 and 79426 files, respectively.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 03:23:41 d2.data.datasets.coco]: \u001b[0mWill use their intersection of 79426 files.\n",
            "\u001b[32m[07/26 03:23:41 d2.data.datasets.coco]: \u001b[0mLoaded 79426 images with semantic segmentation from /content/drive/MyDrive/datasets/coco/train2017\n",
            "\u001b[32m[07/26 03:23:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[07/26 03:23:41 d2.data.common]: \u001b[0mSerializing 79426 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[07/26 03:23:42 d2.data.common]: \u001b[0mSerialized dataset takes 19.92 MiB\n",
            "RESUME:  False\n",
            "\u001b[32m[07/26 03:23:42 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /content/drive/MyDrive/ov-seg2/ovseg_swinbase_vitL14_ft_mpt.pth ...\n",
            "\u001b[32m[07/26 03:23:42 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from /content/drive/MyDrive/ov-seg2/ovseg_swinbase_vitL14_ft_mpt.pth ...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 03:23:45 fvcore.common.checkpoint]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mclip_adapter.clip_model_reg.ln_final.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.token_embedding.weight\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.0.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.0.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.0.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.0.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.0.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.1.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.1.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.1.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.1.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.1.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.10.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.10.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.10.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.10.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.10.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.11.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.11.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.11.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.11.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.11.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.2.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.2.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.2.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.2.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.2.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.3.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.3.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.3.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.3.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.3.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.4.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.4.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.4.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.4.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.4.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.5.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.5.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.5.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.5.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.5.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.6.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.6.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.6.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.6.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.6.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.7.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.7.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.7.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.7.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.7.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.8.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.8.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.8.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.8.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.8.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.9.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.9.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.9.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.9.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.transformer.resblocks.9.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.conv1.weight\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.ln_post.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.ln_pre.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.0.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.0.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.1.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.1.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.10.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.10.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.11.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.11.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.12.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.12.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.13.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.13.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.14.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.14.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.15.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.15.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.16.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.16.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.17.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.17.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.18.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.18.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.19.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.19.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.2.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.2.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.20.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.20.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.21.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.21.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.22.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.22.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.23.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.23.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.3.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.3.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.4.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.4.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.5.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.5.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.6.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.6.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.7.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.7.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.8.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.8.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.9.ln_1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.9.ln_2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.visual.{class_embedding, mask_embedding, positional_embedding, proj}\u001b[0m\n",
            "\u001b[34mclip_adapter.clip_model_reg.{logit_scale, positional_embedding, text_projection}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.mask_tokens.weight\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.norm4.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.norm4.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mclip_adapter.mask_decoder.transformer.norm_final_attn.{bias, weight}\u001b[0m\n",
            "\u001b[32m[07/26 03:23:45 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "> /content/drive/MyDrive/cseg-tfrm/open_vocab_seg/ovseg_model.py(199)forward()\n",
            "-> for i in range(num_batches):\n",
            "(Pdb) --KeyboardInterrupt--\n",
            "(Pdb) ^C\n"
          ]
        }
      ],
      "source": [
        "#@title Train\n",
        "!python /content/drive/cseg/train_net.py --num-gpu 1 --config-file /content/cseg/ovseg_swinB_vitL_bs32_ade20k.yaml  # --resume MODEL.WEIGHTS /content/drive/MyDrive/cseg-tfrm/output/model_0000999.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdEKyDADvkzo"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/cseg-tfrm/train_net.py --num-gpu 1 --config-file /content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml   --resume MODEL.WEIGHTS /content/drive/MyDrive/cseg-tfrm/output/model_0000999.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0mCnWFovlba"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/cseg-tfrm/train_net.py --num-gpu 1 --config-file /content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml   --resume MODEL.WEIGHTS /content/drive/MyDrive/cseg-tfrm/output/model_0000999.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBCmn9kAvl5x"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/cseg-tfrm/train_net.py --num-gpu 1 --config-file /content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml   --resume MODEL.WEIGHTS /content/drive/MyDrive/cseg-tfrm/output/model_0000999.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_iUkzTEvmNy"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/cseg-tfrm/train_net.py --num-gpu 1 --config-file /content/drive/MyDrive/cseg-tfrm/ovseg_swinB_vitL_bs32_coco.yaml   --resume MODEL.WEIGHTS /content/drive/MyDrive/cseg-tfrm/output/model_0000999.pth"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
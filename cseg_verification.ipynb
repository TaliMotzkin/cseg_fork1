{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Set-up"
      ],
      "metadata": {
        "id": "baiyn-5jdy2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2R0SkyHcWR5",
        "outputId": "3d3f0f41-8d7e-4f7e-b703-f87bd6c6d48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone cseg into /content/ folder\n",
        "!git clone https://github.com/noellelaw/cseg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "zz4w6Q0JP4ma",
        "outputId": "c4e1353c-b4a4-4c7c-c027-83f8542ae4df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cseg'...\n",
            "remote: Enumerating objects: 423, done.\u001b[K\n",
            "remote: Counting objects: 100% (342/342), done.\u001b[K\n",
            "remote: Compressing objects: 100% (212/212), done.\u001b[K\n",
            "remote: Total 423 (delta 131), reused 316 (delta 120), pack-reused 81\u001b[K\n",
            "Receiving objects: 100% (423/423), 6.66 MiB | 20.41 MiB/s, done.\n",
            "Resolving deltas: 100% (147/147), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read in KITTI test data from drive (link share: https://drive.google.com/drive/folders/1LLKGeYnLXBY1lJXRKOUpTk4GKZaEoNYR?usp=drive_link)\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "from PIL import Image\n",
        "# Access google drive folder, update to specific path the shared folder is sent\n",
        "data_fldr = '/content/drive/MyDrive/training/image_2/' #@param {kitti_image_fldr:'string'}\n",
        "# Set up data structure to hold images and filenames\n",
        "images = {'filename':[],\n",
        "          'image':[]}\n",
        "# Iterate thru folder to store images and respective filenames\n",
        "for file in os.listdir(data_fldr):\n",
        "  full_file = os.path.join(data_fldr, file)\n",
        "  images['image'].append(Image.open(full_file))\n",
        "  images['filename'].append(file)"
      ],
      "metadata": {
        "id": "ncaY5wLCd6Dr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install and import OV-Seg requirments\n",
        "%cd /content/cseg/\n",
        "import multiprocessing as mp\n",
        "!pip install -r requirements.txt\n",
        "!pip install wandb\n",
        "!pip install timm\n",
        "!pip install ftfy\n",
        "\n",
        "try:\n",
        "    import detectron2\n",
        "except:\n",
        "    import os\n",
        "    os.system('pip install git+https://github.com/facebookresearch/detectron2.git')\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from open_vocab_seg import add_ovseg_config\n",
        "from open_vocab_seg.utils import VisualizationDemo"
      ],
      "metadata": {
        "id": "q_yi9ZiA6i7e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install vlseg_ensembling, needed for benchmark evaluation\n",
        "# better way to do this in the future\n",
        "import os\n",
        "%cd /content/\n",
        "!git clone https://github.com/noellelaw/vlseg_ensembling --recurse-submodules\n",
        "%cd /content/vlseg_ensembling\n",
        "os.mkdir('/content/vlseg_ensembling/kitti_benchmark_suite/devkit/devkit/evaluation/KITTI_RESULTS/results')\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "4i86ga-FeFLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267af30d-51f7-4f80-981f-80497b365c2d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'vlseg_ensembling'...\n",
            "remote: Enumerating objects: 1105, done.\u001b[K\n",
            "remote: Counting objects: 100% (523/523), done.\u001b[K\n",
            "remote: Compressing objects: 100% (444/444), done.\u001b[K\n",
            "remote: Total 1105 (delta 74), reused 456 (delta 54), pack-reused 582\u001b[K\n",
            "Receiving objects: 100% (1105/1105), 161.14 MiB | 17.66 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "Updating files: 100% (1143/1143), done.\n",
            "/content/vlseg_ensembling\n",
            "Collecting git+https://github.com/openai/CLIP.git (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-cxqjlgi7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-cxqjlgi7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.7.0.72)\n",
            "Requirement already satisfied: mss in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (9.0.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.9.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2022.10.31)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.9.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.2.2)\n",
            "Requirement already satisfied: lvis in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 3)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 3)) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 3)) (0.16.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 3)) (0.3.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext->-r requirements.txt (line 7)) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext->-r requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.10/dist-packages (from lvis->-r requirements.txt (line 9)) (0.29.36)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r requirements.txt (line 9)) (1.4.4)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from lvis->-r requirements.txt (line 9)) (3.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r requirements.txt (line 9)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (8.1.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (4.65.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->lvis->-r requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->lvis->-r requirements.txt (line 9)) (4.42.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->lvis->-r requirements.txt (line 9)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->lvis->-r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm->-r requirements.txt (line 3)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm->-r requirements.txt (line 3)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm->-r requirements.txt (line 3)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm->-r requirements.txt (line 3)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm->-r requirements.txt (line 3)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm->-r requirements.txt (line 3)) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm->-r requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm->-r requirements.txt (line 3)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm->-r requirements.txt (line 3)) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm->-r requirements.txt (line 3)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import common libraries\n",
        "%cd /content/cseg\n",
        "import torch\n",
        "import requests\n",
        "import sys\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import math\n",
        "from time import perf_counter\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as mplc\n",
        "import matplotlib.figure as mplfigure\n",
        "from torchvision import transforms\n",
        "import colorsys\n"
      ],
      "metadata": {
        "id": "Xknp57qneJxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac4ffd1-bdf0-40db-d01d-a51925c76178",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cseg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Utils"
      ],
      "metadata": {
        "id": "8hEez17Pgiuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Color map class\n",
        "_OTHER = np.array([ 0, 0, 0 ])\n",
        "_COLORS = np.array([\n",
        "          128,   64, 128,#\n",
        "          244,   35, 232,#\n",
        "          70,    70,  70,#\n",
        "          102,  102, 156,#\n",
        "          190,  153, 153,#\n",
        "          153,  153, 153,#\n",
        "          250,  170,  30,#\n",
        "          220,  220,   0,#\n",
        "          107,  142,  35,#\n",
        "          152,  251, 152,#\n",
        "            70, 130, 180,#\n",
        "          220,   20,  60,#\n",
        "          255,    0,   0,#\n",
        "            0,    0, 142,#\n",
        "            0,    0,  70,#\n",
        "            0,   60, 100,#\n",
        "            0,   80, 100,#\n",
        "            0,    0, 230,#\n",
        "          119,   11,  32,#\n",
        "            0,    0,   0,\n",
        "            0,    0,   0,\n",
        "          ]).reshape(-1, 3)\n",
        "# BASED OFF KITTI BENCHMARK SUITE LABEL MAPPING\n",
        "_GRAYS = np.array([\n",
        "          7,\n",
        "          8,\n",
        "          11,\n",
        "          12,\n",
        "          13,\n",
        "          17,\n",
        "          19,\n",
        "          20,\n",
        "          21,\n",
        "          22,\n",
        "          23,\n",
        "          24,\n",
        "          25,\n",
        "          26,\n",
        "          27,\n",
        "          28,\n",
        "          31,\n",
        "          32,\n",
        "          33,\n",
        "          0,\n",
        "          0\n",
        "          ]).reshape(-1, 1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dXfKpRDUglk0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Overlay background mask on image\n",
        "from typing import Tuple\n",
        "# Overlay for 3 Channel RGB images\n",
        "def overlay_rgb(\n",
        "    image: np.ndarray,\n",
        "    mask: np.ndarray,\n",
        "    color: Tuple[int, int, int] = (255, 0, 0),\n",
        "    alpha: float = 0.6,\n",
        "    resize: Tuple[int, int] = (1242,375)\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Combines image and its segmentation mask into a single image.\n",
        "\n",
        "    Params:\n",
        "        image: Training image.\n",
        "        mask: Segmentation mask.\n",
        "        color: Color for segmentation mask rendering.\n",
        "        alpha: Segmentation mask's transparency.\n",
        "        resize: If provided, both image and its mask are resized before blending them together.\n",
        "\n",
        "    Returns:\n",
        "        image_combined: The combined image.\n",
        "\n",
        "    \"\"\"\n",
        "    color = np.asarray(color).reshape(3, 1, 1)\n",
        "    colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)\n",
        "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
        "    image_overlay = masked.filled()\n",
        "\n",
        "    if resize is not None:\n",
        "        image = cv2.resize(image.transpose(1, 2, 0), resize)\n",
        "        image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)\n",
        "\n",
        "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
        "    return image_combined\n",
        "# Overlay for gray scale images\n",
        "def overlay(\n",
        "    image: np.ndarray,\n",
        "    mask: np.ndarray,\n",
        "    color: int = 255,\n",
        "    alpha: float = 0.6,\n",
        "    resize: Tuple[int, int] = (1242,375)\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Combines image and its segmentation mask into a single image.\n",
        "\n",
        "    Params:\n",
        "        image: Training image.\n",
        "        mask: Segmentation mask.\n",
        "        color: Color for segmentation mask rendering.\n",
        "        alpha: Segmentation mask's transparency.\n",
        "        resize: If provided, both image and its mask are resized before blending them together.\n",
        "\n",
        "    Returns:\n",
        "        image_combined: The combined image.\n",
        "\n",
        "    \"\"\"\n",
        "    color = np.asarray(color).reshape(1, 1, 1)\n",
        "    colored_mask = np.expand_dims(mask, 0).repeat(1, axis=0)\n",
        "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
        "    image_overlay = masked.filled()\n",
        "\n",
        "    if resize is not None:\n",
        "        image = cv2.resize(image, resize)\n",
        "        image_overlay = cv2.resize(image_overlay, resize)\n",
        "\n",
        "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
        "\n",
        "    return image_combined"
      ],
      "metadata": {
        "id": "Pz9SzVlsgpyS",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build Model"
      ],
      "metadata": {
        "id": "tlawIO88aJk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils\n",
        "import torch\n",
        "from time import perf_counter\n",
        "def setup_cfg(config_file, model_weights):\n",
        "    # load config from file and command-line arguments\n",
        "    cfg = get_cfg()\n",
        "    add_deeplab_config(cfg)\n",
        "    add_ovseg_config(cfg)\n",
        "    cfg.merge_from_file(config_file)\n",
        "    cfg.MODEL.WEIGHTS = model_weights\n",
        "    cfg.freeze()\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def inference(class_names, input_img, demo):\n",
        "\n",
        "    print(\"Getting ovseg predictions ... \")\n",
        "    t1_start = perf_counter()\n",
        "    output = {\n",
        "        'pred_masks': [],\n",
        "        'pred_classes': [],\n",
        "        'scores' : [],\n",
        "        }\n",
        "    class_names = class_names.split(',')\n",
        "    preds = demo.run_on_image(input_img, class_names)[0]\n",
        "    if \"sem_seg\" in preds:\n",
        "      r = preds[\"sem_seg\"]\n",
        "      blank_area = (r[0] == 0)\n",
        "      pred_mask = r.argmax(dim=0).to('cpu')\n",
        "      probs = r.softmax(dim=0).detach().cpu()\n",
        "      confidence, _ = probs.max(dim=0)\n",
        "      pred_mask[blank_area] = 255\n",
        "      # Pred mask of 'dead' area\n",
        "      pred_mask = np.array(pred_mask, dtype=int)\n",
        "      pred_classes, pred_masks =  get_sem_seg_masks(\n",
        "          sem_seg = pred_mask,\n",
        "          class_names = class_names,\n",
        "      )\n",
        "      output['pred_classes'].append(pred_classes)\n",
        "      output['pred_masks'].append(pred_masks)\n",
        "      output['scores'].append(confidence.numpy())\n",
        "    tfinal = perf_counter() - t1_start\n",
        "    print(\"Returning clipseg predictions: {:.2f} seconds\".format(tfinal))\n",
        "    return output, tfinal\n",
        "\n",
        "def get_sem_seg_masks(sem_seg, class_names, area_threshold=None):\n",
        "        if isinstance(sem_seg, torch.Tensor):\n",
        "            sem_seg = sem_seg.numpy()\n",
        "        labels, areas = np.unique(sem_seg, return_counts=True)\n",
        "        sorted_idxs = np.argsort(-areas).tolist()\n",
        "        labels = labels[sorted_idxs]\n",
        "        pred_classes, pred_masks = [], []\n",
        "\n",
        "        for label in filter(lambda l: l < len(class_names), labels):\n",
        "            # I need to find a way to assign confidence\n",
        "            binary_mask = (sem_seg == label).astype(np.uint8)\n",
        "            # text = class_names[label]\n",
        "            # Storing index instead of text\n",
        "            pred_classes.append(label)\n",
        "            pred_masks.append(binary_mask)\n",
        "        return pred_classes, pred_masks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k1shrZC4cUyN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build model\n",
        "mp.set_start_method(\"spawn\", force=True)\n",
        "config_file = '/content/cseg/configs/ovseg_swinB_vitL_demo.yaml' #@param {cofig_file: \"string\"}\n",
        "model_weights = '/content/drive/MyDrive/model_weights/output_2layers/model_0000999_hpc.pth' #@param {model_weights: \"string\"}\n",
        "cfg = setup_cfg(config_file, model_weights)\n",
        "demo = VisualizationDemo(cfg)\n",
        "\n"
      ],
      "metadata": {
        "id": "fjSUafRzcSPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "496d9513-ce75-41c5-b717-d692fdfbdab6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.config:Loading config /content/cseg/configs/ovseg_swinB_vitL_demo.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "2OX8Ipxd3XLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define prompt string\n",
        "prompts = 'road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, terrain, sky, person, rider, car, truck, bus, train, motorcycle, bicycle'\n",
        "prompt_list = prompts.split(', ')"
      ],
      "metadata": {
        "id": "dvHPUKQxfocg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get all cseg predictions\n",
        "pred_data = {'filename': [],\n",
        "              'image': [],\n",
        "              'ovseg': [],\n",
        "              'time': []}\n",
        "\n",
        "pred_data['final'] = []\n",
        "pred_data['final_rgb'] = []\n",
        "# Folder to save for KITTI evaluation, don't change\n",
        "eval_fldr = '/content/vlseg_ensembling/kitti_benchmark_suite/devkit/devkit/evaluation/KITTI_RESULTS/results/'\n",
        "# Folders in your drive to save to\n",
        "# grayscale predictions\n",
        "pred_fldr = '/content/drive/MyDrive/zeroshot/cseg2/' #@param {prediction_folder:\"string\"}\n",
        "# RGB predictions\n",
        "rgb_fldr = '/content/drive/MyDrive/zeroshot/cseg2_rgb/' #@param {rgb_prediction_folder:\"string\"}\n",
        "# COnfidence predictions\n",
        "conf_fldr = '/content/drive/MyDrive/zeroshot/cseg2_conf/'#@param {confidence_prediction_folder:\"string\"}\n",
        "\n",
        "for i, image in enumerate(images['image']):\n",
        "  print(f'Processing image {i}.')\n",
        "  np_img = np.array(image).astype('uint8')\n",
        "  pred_data['filename'].append(images['filename'][i])\n",
        "  pred_data['image'].append(images['image'][i])\n",
        "  preds, time = inference(prompts, np_img, demo)\n",
        "\n",
        "  ovseg_classes = preds['pred_classes']\n",
        "  ovseg_masks = preds['pred_masks']\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Overlay clipseg masks on image\n",
        "  height, width, _ = np.array(image).shape\n",
        "  image = np.multiply(np.ones((height, width)), 0)\n",
        "  # Get RGB Image -----------------------------------------------------\n",
        "  image_rgb = np.multiply(np.ones((height, width, 3)), 0)\n",
        "  for j, mask in enumerate(preds['pred_masks'][0]):\n",
        "    color = _COLORS[preds['pred_classes'][0][j]]\n",
        "    image_rgb = overlay_rgb(\n",
        "        image = image_rgb.transpose(2,0,1),\n",
        "        mask = mask,\n",
        "        color = tuple(color),\n",
        "        resize = tuple([width,height]),\n",
        "        alpha = 1\n",
        "    )\n",
        "  # Get Gray Image -----------------------------------------------------\n",
        "  for j, mask in enumerate(preds['pred_masks'][0]):\n",
        "    color = _GRAYS[preds['pred_classes'][0][j]]\n",
        "    image = overlay(\n",
        "        image = image,\n",
        "        mask = mask,\n",
        "        color = tuple(color),\n",
        "        resize = tuple([width,height]),\n",
        "        alpha = 1\n",
        "    )\n",
        "  pred_data['final'].append(image.astype('uint8'))\n",
        "  pred_data['final_rgb'].append(image_rgb.astype('uint8'))\n",
        "  # Save images to google drive\n",
        "  filename_eval = eval_fldr+pred_data['filename'][i]\n",
        "  filename_pred = pred_fldr+pred_data['filename'][i]\n",
        "  filename_rgb = rgb_fldr+pred_data['filename'][i]\n",
        "  filename_conf = conf_fldr+pred_data['filename'][i]\n",
        "  image_eval = Image.fromarray(image.astype('uint8'))\n",
        "  image_eval.save(filename_eval)\n",
        "  image_pred = Image.fromarray(image.astype('uint8'))\n",
        "  image_pred.save(filename_pred)\n",
        "  image_rgb = Image.fromarray(image_rgb.astype('uint8'))\n",
        "  image_rgb.save(filename_rgb)\n",
        "  image_conf = Image.fromarray(np.array(preds['scores'][0]*255).astype('uint8'))\n",
        "  image_conf.save(filename_conf)\n",
        "  #break\n"
      ],
      "metadata": {
        "id": "ErGo0oTYfSp6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KITTI Evaluation"
      ],
      "metadata": {
        "id": "Jg73YIMVYbNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform KITTI Evaluation\n",
        "! python /content/vlseg_ensembling/kitti_benchmark_suite/devkit/devkit/evaluation/evalPixelLevelSemanticLabeling.py /content/output/"
      ],
      "metadata": {
        "id": "USj-M_Lzs0tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e9f675-63fd-4f8b-d6d2-9370f6549e99"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Found no prediction for ground truth /content/vlseg_ensembling/kitti_benchmark_suite/devkit/devkit/evaluation/KITTI_RESULTS/training/semantic/000125_10.png\n"
          ]
        }
      ]
    }
  ]
}